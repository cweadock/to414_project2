---
title: "Project 2"
author: "Kate Sawdey, Claire Weadock, Olivia Caponecchi, Michael Zuckerman, Anna Dolce, Rubens Mondi"
date: "3/5/2023"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
    number_sections: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Executive Summary

**[insert exec summary]**

# Intro

**[insert intro]**

- introduce problem and process to solve

- current success rate = 11.26%
- current profit = -13,348
- break even success rate = 16.667%


## Downloading and Prepping the Data

Professor Kumar had given us a starter rmd file with the relevant data already loaded, cleaned, and normalized. The corresponding code blocks are displayed below.

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

summary(tele$y) #this was used to calculate some of the statistics in the intro


```



## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))
```


## Getting Train and Test Samples

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), 10000) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# we only want the labels taken out of the data set for the knn model, the other two we can use a test/train break up with all variables included.
log_train <- tele_norm[ -test_set, ]
log_test <- tele_norm[ test_set , ]

# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_norm[-test_set, -match("yyes",names(tele_norm))]
tele_test <- tele_norm[test_set, -match("yyes",names(tele_norm))]

#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_norm[-test_set, "yyes"]
tele_test_labels <- tele_norm[test_set, "yyes"]

# can use write.csv to put all of our new data sets into seperate, cleaned csv
#write.csv(tele_train, "tele_train.csv")
```


# Logistic Regression

See the following code along with comments which outline the process we followed to create a logistic regression model with the call center data.

```{r log model main effects}

log_model_ME <- glm(yyes ~ . , data = log_train , family = "binomial")



summary(log_model_ME)

log_pred_ME <- predict(log_model_ME , log_test , type = "response")

log_pred_ME_binary <- ifelse(log_pred_ME >= 0.3, 1, 0) # have to find the right threshold here; may be useful to find break even rates


library(caret)
confmat <- confusionMatrix(as.factor(log_pred_ME_binary) , as.factor(log_test$yyes), positive = "1")

confmat
outcome_weights <- matrix(
          c(0, 0,
          -1, 5),
          nrow = 2,
          byrow = TRUE
)

conf_mat_wg <- sum(confmat$table*outcome_weights)
str(conf_mat_wg)

outcome <- data.frame(threshold = numeric(), profit = numeric())


#We have created  a for loop that calculate the profit level for each threshold in the range from 0 to 1 with steps of 0.01. 

threshold_list <- seq(from = 0.01, to = 1, by = 0.01)
for (i in threshold_list) {
  log_pred_ME_binary <- ifelse(log_pred_ME >= i, 1, 0)
  confmat <- confusionMatrix(as.factor(log_pred_ME_binary) , as.factor(log_test$yyes), positive = "1")
  out <- sum(confmat$table*outcome_weights)
  nrow <- data.frame(threshold = i, profit = out)
  outcome <- rbind(outcome,nrow)
}
#the loop resulted in the outcome data-frame, which contain the bottom line profit for each threshold level. 
# We have build a line plot to visualize the result form the loop, from which is apparent that the optimal threshold resides between 0.15 and 0.25. As a matter of fact, a threshold of 0.19 return a profit of 2493 $, which is the highest possible. 
#NB. It is possible to take into account the opportunity cost of losing the call for which we predicted a negative outcome but which actually could have been resulted in a loan subscription. In that case we should adjust the outcome_weights matrix for a value x in c(0, x , -1, 5) that accounts for the opportunity cost (-5<x<0). 
outcome
ggplot(outcome, aes(x = threshold, y = profit)) + 
  geom_line() + 
  labs(x = "Threshold", y = "Profit", title = "Profit per Treshold level")

#With the optimal threshold value we have: 
log_pred_ME_binary <- ifelse(log_pred_ME >= 0.19, 1, 0)
opt_confmat <- confusionMatrix(as.factor(log_pred_ME_binary) , as.factor(log_test$yyes), positive = "1") 
opt_confmat #this is the confusion matrix before applying weights 

opt_conf_mat_wg <- confmat$table*outcome_weights 
opt_conf_mat_wg #this is the confusion matrix after applying the weights 
```




# KNN Model

The first KNN model we tried was simple with k=1 nearest neighbors. This gives us an idea of a starting point to determine the optimal KNN model for the call center to use.

```{r first knn, cache=TRUE}

library(class)
knn_model_first <- knn(tele_train , tele_test , cl = tele_train_labels, k = 1)



```

```{r}

library(caret)
confusionMatrix(as.factor(knn_model_first) , as.factor(tele_test_labels) , positive = "1")

```

As shown in the confusion Matrix, this KNN model is missing a large amount of successful calls (false negatives) and therefore is missing out on a large amount of potential profit. To determine the optimal K value we can run various for loops which test different k values for both accuracy and profitability (taking opportunity cost into account). 


## Finding optimal k value based on model accuracy

```{r knn optimal accuracy, cache=TRUE}
accuracies<- c()
for (i in 1:10){ 
    knn.mod <-  knn(train=tele_train, test=tele_test, cl=tele_train_labels, k=i)
    optm_k <- 100 * sum(tele_test_labels == knn.mod)/NROW(tele_test_labels)
    k=i  
    accuracies <- append(accuracies, optm_k)
    cat(k,'=',optm_k,'\n')       # to print % accuracy 
}
```
```{r}
plot(accuracies, type = "o", xlab="k value", ylab = "Accuracy %")
```


## Finding financially optimal k value


```{r}
library(gmodels)

```


```{r knn optimal profits, cache=TRUE}
profits <- c()

for (i in 1:10){
  knn.mod2 <-  knn(train=tele_train, test=tele_test, cl=tele_train_labels, k=i)
  crosstab <- CrossTable(knn.mod2 , tele_test_labels, prop.r = FALSE, prop.c = FALSE, prop.t = FALSE, prop.chisq = FALSE)
  A <- crosstab$t['0','0']
  B <- crosstab$t['1','0']
  C <- crosstab$t['0','1']
  D <- crosstab$t['1','1']
  
  profit <- (A)-(5*B)-(C)+(5*D)
  profits <- append(profits, profit)
  
  cat('k =', i, 'then profit = $', profit, '\n')
}
```

```{r}
plot(profits, type = "o", xlab="k value", ylab = "Profit $")

```

You can see that if you were to look soley based on the accuracy of the model, the optimal k would have been 7. However, the scope of this project is to make the call center the most profitable rather than make the model the most accurate, therefore we can see that the true optimal k once profitability and opportunity cost are taken into account is k=9. Below is the code for a KNN model using the optimal k value of 9.

```{r optimal knn model}
library(class)
optimal_knn_pred <- knn(tele_train , tele_test , cl = tele_train_labels, k = 9)

library(class)
confusionMatrix(as.factor(optimal_knn_pred) , as.factor(tele_test_labels) , positive = "1")

```



# ANN Model

Per our discussion with Professor Kumar's, we attempted to run an ANN model on the data using 1, 5, and 20 as potential hidden values. In the first two code blocks below, you can see our code which runs an ANN model using both 1 and 5 for hidden values and saving them to RDS files respectively. However, due to the limited computing power of our personal laptops, we were unable to get an ANN model with hidden = 20 to run successfully. The code which we unsuccessfully attempted is also illustrated below in the third code block.

We chose to save each model into an RDS file then mark that code block as eval=FALSE because it prevents the computer from having to re-run the ANN model each time the file is knitted while still being able to keep the entire project on one cohesive rmd and html. 

## ANN with 1 Hidden

Below is the code which generates an ANN model with hidden = 1.

```{r ann 1 hidden, eval=FALSE}


library(neuralnet)
library(caret)

ann_model_1hidden <- neuralnet(yyes ~ . , data = log_train, hidden = 1)
saveRDS(ann_model_1hidden , "ann_1hidden.rds")

# do 1 hidden, then 5, then 20 if possible


```

## ANN with 5 Hidden

Below is the code which generates an ANN model with hidden = 5.

```{r ann 5 hidden, eval=FALSE}

library(neuralnet)
library(caret)

ann_model_5hidden <- neuralnet(yyes ~ . , data = log_train, hidden = 5)
saveRDS(ann_model_5hidden , "ann_5hidden.rds")


```

## ANN with 20 Hidden

Below is the code which would have generated an ANN model with hidden = 20, had our computers had enough computing resources to build the train the model.

```{r ann 20 hidden, eval=FALSE}

library(neuralnet)
library(caret)

ann_model_20hidden <- neuralnet(yyes ~ . , data = log_train, hidden = 20)
saveRDS(ann_model_20hidden , "ann_20hidden.rds")



```


## ANN Prediction

Now that we have an ANN model with a hidden = 5, we can use that model to predict outcomes using out test data. This follows the same process as usual by first loading the RDS file into the code bloc, then using the predict function, and finally generating a confusion matrix to visualize the accuracy of our prediction against the real outcomes in the test data. 

```{r 5 hidden prediction}

hidden5_RDS <- readRDS("ann_5hidden.rds")
pred_5hidden <- predict(hidden5_RDS , log_test)
# we need to optimize the profits based on what we have for the threshold under binarypred_5hidden
binarypred_5hidden <- ifelse(pred_5hidden >= 0.5 , 1, 0)

library(caret)
confusionMatrix(as.factor(binarypred_5hidden) , as.factor(log_test$yyes) , positive = "1")

```

In order for the call center to maximize profits using this model, we need to determine the optimal threshold for creating the binary 0 and 1 prediction of if they should call or not. Like in the logistic regression, it is important that we do not only care about overall accuracy, but also the sensitivity vs the specificity. In this case we want to boost the sensitivity as much as we can without sacrificing too much specificity. This is because from a business standpoint, each successful call gains 6 dollars in revenue whereas each unsuccessful call only costs 1 dollar. Therefore, we care much more about catching as many 1s as we can in this model at the expense of picking up some false positives in our prediction. See the following section for code which optimizes the threshold.


### Optimizing Binary Prediction Threshold to Maximize Profits

As stated above, we need to code a for loop to find the threshold which will maximize profits for the call center. This process followed directly mirrors that in the logistic regression model. 

```{r}


library(caret)
confmat_ann <- confusionMatrix(as.factor(binarypred_5hidden) , as.factor(log_test$yyes), positive = "1")

confmat_ann
outcome_weights_ann <- matrix(
          c(0, 0,
          -1, 5),
          nrow = 2,
          byrow = TRUE
)


conf_mat_wg_ann <- sum(confmat_ann$table*outcome_weights_ann)
str(conf_mat_wg_ann)

outcome_ann <- data.frame(threshold = numeric(), profit = numeric())

threshold_list_ann <- seq(from = 0.01, to = 1, by = 0.01)

for (i in threshold_list_ann) {
  binarypred_5hidden <- ifelse(pred_5hidden >= i, 1, 0)
  confmat_ann <- confusionMatrix(as.factor(binarypred_5hidden) , as.factor(log_test$yyes), positive = "1")
  out <- sum(confmat_ann$table*outcome_weights_ann)
  nrow <- data.frame(threshold = i, profit = out)
  outcome_ann <- rbind(outcome_ann,nrow)
}

outcome_ann
ggplot(outcome_ann, aes(x = threshold, y = profit)) + 
  geom_line() + 
  labs(x = "Threshold", y = "Profit", title = "Profit per Treshold level")

# the optimal threshold looking at the table and plot is 0.11, so let's generate a prediction and confusionMatrix using said value

binarypred_5hidden <- ifelse(pred_5hidden >= 0.11 , 1, 0)

library(caret)
confusionMatrix(as.factor(binarypred_5hidden) , as.factor(log_test$yyes) , positive = "1")


```

Therefore, the optimal threshold for the ANN model prediction is 0.11. 


# Combining Models Together

To combine the three models together, we created a new variable which was a sum of all the predictions from the three different models. To do this, we first had to convert the KNN predictions into a number (from a factor), and then we can sum them using simple addition. Next, we created a new variable, binary_combined_call which was equal to 1 if at least 2 models predicted a successful call, else 0. Below you can see the code we used for this process and the confusion matrix comparing the predicted outcome to real outcomes with the test data using all three models combined.

```{r}

#first we have to convert optimal_knn_pred into a number

optimal_knn_pred <- as.numeric(as.character(optimal_knn_pred))

summed_predictions <- optimal_knn_pred + binarypred_5hidden + log_pred_ME_binary

binary_combined_call <- ifelse(summed_predictions >= 2 , 1, 0)

library(caret)
confusionMatrix(as.factor(binary_combined_call) , as.factor(log_test$yyes) , positive = "1")

```




# Results and Analysis

**[INSERT RESULTS AND ANALYSIS]**

# Conclusion

**[INSERT CONCLUSION]**
